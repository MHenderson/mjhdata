[
  {
    "objectID": "posts/2019-12-10-tidy-tuesday-replicating-plots-in-r/index.html",
    "href": "posts/2019-12-10-tidy-tuesday-replicating-plots-in-r/index.html",
    "title": "Tidy Tuesday: Replicating Plots in R",
    "section": "",
    "text": "Tidy Tuesday in week fifty of 2019 was all about this blog post by Rafael Irizarry on replicating plots in R.\nI chose to focus on a heatmap showing infectious diseases in the United States before and after the introduction of vaccines.\nMy plot is heavily based on Rafael’s code with just a few extra annotations and a change of scale to match the original version published in the Wall Street Journal.\nThis was my first attempt at adding annotations outside the main plotting area. I learned how to create the annotations I wanted for this plot from the Tidy Tuesday work of Georgios Karamanis. Especially his week 36 entry for 2021.\nMy code is here: https://github.com/MHenderson/replicating-plots-in-r\n\n\n\nPlot shows a heat map of measles cases per one hundred thousand people measured from 1928 to 2003 across all fifty US states and the District of Columbia. Showing that after 1963 when measles vaccine was introduced there was a dramatic drop in the number of measles cases throughout the United States."
  },
  {
    "objectID": "posts/2021-05-11-tidy-tuesday-internet-access/index.html",
    "href": "posts/2021-05-11-tidy-tuesday-internet-access/index.html",
    "title": "Tidy Tuesday: Internet Access",
    "section": "",
    "text": "This week’s Tidy Tuesday data comes from two sources, Microsoft and the FCC by way of The Verge.\nAccording to The Verge, the FCC data, which dates from the end of 2017, is\n\na notoriously inaccurate survey drawn from ISPs’ own descriptions of the areas they serve.\n\nMicrosoft estimate connection speeds from throughput of software updates.\n\nWe know the size of the package sent to the computer, and we know the total time of the download. We also determine zip code level location data via reverse IP. Therefore, we can count the number of devices that have connected to the internet at broadband speed per each zip code based on the FCC’s definition of broadband that is 25mbps per download.\n\nMicrosoft’s data is from November 2019.\nI chose to compare these two different measures of connection speed at county level for one specific state, Kentucky.\nTo plot county boundaries I used the {tigris} package, an R package for downloading geographic data from the United States Census Bureau.\nThis was first time I can remember using {tigris}. It made plotting county boundaries a breeze.\n{tigris} can also be used to plot other data from the Census Bureau. I got a bit distracted plotting maps of Kentucky roads.\nFor example, to download all roads in Madison County and plot them with {ggplot2} you can do the following:\nlibrary(ggplot2)\nlibrary(tigris)\n\nmadison_ky_roads &lt;- roads(\"KY\", \"Madison\", progress_bar = FALSE)\n\nmadison_ky_roads %&gt;%\n  ggplot() +\n  geom_sf(size = .1, alpha = .8) +\n  theme_void()\n\nIn the final plot below there are two maps of the counties of Kentucky.\nIn the top map, colours correspond to the proportion of residents in a county that have access to broadband internet, according to the FCC.\nThe colouring in the bottom map represents estimates of the same proportion, according to Microsoft.\nThe two plots differ substantially.\nAccording to the first plot, the proportion of residents having access to broadband internet appears to be high in most counties.\nHowever, the second plot suggests that the actual proportion of people in counties using the internet at broadband speed is much lower, except perhaps in the vicinity of cities like Lexington, Louisville, Bowling Green and Cincinnati.\nSource code: https://github.com/MHenderson/internet-access\n\n\n\nA plot comparing different ways of measuring internet access for people living in Kentucky"
  },
  {
    "objectID": "posts/2021-05-05-tidy-tuesday-water-sources/index.html",
    "href": "posts/2021-05-05-tidy-tuesday-water-sources/index.html",
    "title": "Tidy Tuesday: Water Sources",
    "section": "",
    "text": "This week’s Tidy Tuesday data comes from Water Point Data Exchange (WPDx), a global platform for sharing water point data.\nInspired by David Robinson’s livestream on 4/5/21 I created a faceted map plot showing the locations of different water sources throughout Ethiopia.\n{{% youtube “5ub92c-5xFQ” %}}\nThis was my first time using Thomas Lin Pedersen’s {ragg} package. It allowed me to use one of my favourite fonts, Cardo.\nI was also inspired by the work of Georgios Karamanis and a tweet by Nicola Rennie.\nSource code: https://github.com/MHenderson/water-sources"
  },
  {
    "objectID": "posts/2023-05-09-tidy-tuesday-childcare-costs/index.html",
    "href": "posts/2023-05-09-tidy-tuesday-childcare-costs/index.html",
    "title": "Tidy Tuesday: Childcare Costs",
    "section": "",
    "text": "The plot below is my attempt to recreate in {{ggplot2}} one of the plots at https://www.dol.gov/agencies/wb/topics/childcare/price-by-age-care-setting. Namely the plot that greets you when you follow the above link, with the price selector dropdown at the default value of “Infant center-based”.\n\n\n\nA chloropleth map showing the median cost of childcare in the USA between 2016 and 2018. The coloured regions of the map correspond to counties.\n\n\nI faced one or two difficulties in trying to recreate the original plot. One issue I didn’t resolve was how to adjust the prices for CPI-U (the Consumer Price Index for All Urban Consumers). It was easy enough to figure out what calculation needed to be done but I couldn’t find a source of CPI-U for childcare costs in the USA between 2016 and 2018.\nAnother issue I failed to resolve was how to add maps of Alaska and Hawaii to the plot of the rest of the USA.\nThe code behind my plot is here: https://github.com/MHenderson/childcare-costs"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mjhdata",
    "section": "",
    "text": "Tidy Tuesday: Tornadoes\n\n\n\n\n\n\ndataviz\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: Childcare Costs\n\n\n\n\n\n\ndataviz\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Examples of Maximal Room Squares in R\n\n\n\n\n\n\ncombinatorics\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: Internet Access\n\n\n\n\n\n\ndataviz\n\n\n\nWeek 21 of 2021.\n\n\n\n\n\nMay 11, 2021\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: Water Sources\n\n\n\n\n\n\ndataviz\n\n\n\n\n\n\n\n\n\nMay 5, 2021\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: Adoptable Dogs\n\n\n\n\n\n\ndataviz\n\n\n\nWeek 51 of 2019.\n\n\n\n\n\nDec 17, 2019\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: Replicating Plots in R\n\n\n\n\n\n\ndataviz\n\n\n\nWeek 50 of 2019.\n\n\n\n\n\nDec 10, 2019\n\n\nMatthew Henderson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "",
    "text": "In this post I’ll show you how to use Google Cloud Platform’s Vision API to extract text from images in R.\nI’m mostly interested in the case when the text being extracted is handwritten. For typewritten or printed text I use Tesseract.\nIs it feasible to use Vision API for this task? Tesseract works very well with typewritten or printed text but does not seem to handle handwriting as well.[^1] Google’s Vision API, on the other hand, is able to extract handwritten text from images."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#creating-document-text-detection-requests",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#creating-document-text-detection-requests",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "Creating document text detection requests",
    "text": "Creating document text detection requests\nTo use Vision API to extract text from images we must send images to the URL https://vision.googleapis.com/v1/images:annotate in JSON format. Putting images into JSON means encoding them as text. Vision API requires images encoded as text to use the Base64 encoding scheme. (see https://en.wikipedia.org/wiki/Base64 for more details).\nThe JSON payload of a text detection request to Vision API should look something like this:\n{\n  \"requests\": [\n    {\n      \"image\": {\n        \"content\": &lt;encoded-image&gt;\n      },\n      \"features\": [\n        {\n          \"type\": \"DOCUMENT_TEXT_DETECTION\"\n        }\n      ]\n    }\n  ]\n}\nwhere &lt;encoded-image&gt; is a Base64 encoding of the image we want to extract text from.\nFortunately, you don’t have to figure out how to do the encoding yourself. The R package {base64enc} (Urbanek (2015)) can do that for you.\nThe document_text_detection function below uses base64enc::base64encode to compute a Base64 encoding of the image specified by the input path. It then packs the resulting encoding into a list of the right format for converting to JSON before being posted to the URL above.\ndocument_text_detection &lt;- function(image_path) {\n  list(\n    image = list(\n      content = base64enc::base64encode(image_path)\n    ),\n    features = list(\n      type = \"DOCUMENT_TEXT_DETECTION\"\n    )\n  )\n}\nAs you will be using {httr} (Wickham (2020)) you don’t even have to worry about doing the conversion to JSON yourself. {httr} will do it automatically using {jsonlite} (Ooms (2014)) behind-the-scenes when you make your request.\nNow you are ready to use document_text_detection to create a request based on the image located at: ~/workspace/baree-handwriting-scans/001.png\ndtd_001 &lt;- document_text_detection(\"~/workspace/baree-handwriting-scans/001.png\")\nBe wary of inspecting the return value of this function call. It contains a huge base64 string and R might take a very long time to output the string to the screen.\nYou can use substr to inspect part of it, if you really want.\nsubstr(dtd_001, 1, 200)\n#&gt; [1] \"list(content = \\\"iVBORw0KGgoAAAANSUhEUgAAE2EAABtoCAIAAAA8pmPCAAAAA3NCSVQICAjb4U/gAAAACXBIWXMAAFxGAABcRgEUlENBAAAgAElEQVR4nOzdwW7aQBRA0dB/9E8yH+kuUC1qCJiUxFx6zgIJMRo/GxaJzBWH4/E4xvj4+Dg9nkzTNE3T+dPVgu3O\"\n#&gt; [2] \"list(type = \\\"DOCUMENT_TEXT_DETECTION\\\")\"\nAnother pitfall to be wary of is accidentally encoding the path to a file instead of the contents of the file itself. If the response from the Vision API has error messages containing paths then this can be a possible cause."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#posting-requests-to-vision-api",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#posting-requests-to-vision-api",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "Posting requests to Vision API",
    "text": "Posting requests to Vision API\nNow you can use httr:POST from {httr} to post your request to the Vision API.\nhttr::POST requires at least url and body arguments.\nurl is the webpage to be retrieved. In this case \"https://vision.googleapis.com/v1/images:annotate\"\nbody is the request payload. In this case a named list with one element named requests whose value is a list of request objects.\nAs well as those required arguments you also have to tell the server that the content of your request is JSON. You do this by adding “Content-Type: application/json” to the header of your request. This means passing a call to httr::content_type_json() as one of the optional arguments to httr::POST.\nThe Vision API documentation says that for a request to be accepted it must have an Authorization = \"Bearer &lt;GCLOUD_ACCESS_TOKEN&gt;\" header. We can add the header using the httr::add_headers function.\nhttr::add_headers(\n    \"Authorization\" = paste(\"Bearer\", Sys.getenv(\"GCLOUD_ACCESS_TOKEN\"))\n)\nHere we use Sys.getenv(\"GCLOUD_ACCESS_TOKEN\") to obtain the value of our access token.\nThe post_vision function below takes a requests list as input and returns the response from Vision API’s annotation endpoint as a list.\npost_vision &lt;- function(requests) {\n  httr::POST(\n    url = \"https://vision.googleapis.com/v1/images:annotate\",\n    body = requests,\n    encode = \"json\",\n    httr::content_type_json(),\n    httr::add_headers(\n      \"Authorization\" = paste(\"Bearer\", Sys.getenv(\"GCLOUD_ACCESS_TOKEN\"))\n    )\n  )\n}\nFinally, you are ready to use post_vision to make your first request.\npost_vision expects a list of requests as input. In this case you only have one request dtd_001 which you created earlier by calling document_text_detection with the path to your image. Nevertheless, you still have to pack your single request inside a list.\nl_r_001 &lt;- list(requests = dtd_001)\nCalling post_vision now sends your image to Vision API and returns the response.\nr_001 &lt;- post_vision(l_r_001)\nYou can use httr::status_code to check that a valid response was received.\nhttr::status_code(r_001)\n#&gt; [1] 200\nThe value should be 200.\nIf you get a 401 instead then inspect the value of r_001. If you see something like this:\nResponse [https://vision.googleapis.com/v1/images:annotate]\n  Date: 2021-10-29 09:29\n  Status: 401\n  Content-Type: application/json; charset=UTF-8\n  Size: 634 B\n{\n  \"error\": {\n    \"code\": 401,\n    \"message\": \"Request had invalid authentication credentials. Expected OAuth 2 acc...\n    \"status\": \"UNAUTHENTICATED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n        \"reason\": \"ACCESS_TOKEN_EXPIRED\",\n        \"domain\": \"googleapis.com\",\nthen you might need to rerun the gcloud tool (see section 4 of the appendix) to generate a new access token.\nIn the next section I’ll explain how to get text out of a valid response."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#getting-text-from-the-response",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#getting-text-from-the-response",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "Getting text from the response",
    "text": "Getting text from the response\nIf the response is valid then you can use httr::content to extract the content of the response.\ncontent_001 &lt;- httr::content(r_001)\nThe content contains a lot of information. The text is contained inside responses[[1]] as fullTextAnnotation$text.\nbaree_hw_001 &lt;- content_001$responses[[1]]$fullTextAnnotation$text\ncat(baree_hw_001)\n#&gt; a\n#&gt; vast\n#&gt; fear\n#&gt; To Parce, for many days after he was\n#&gt; born, the world was a\n#&gt; gloony cavern.\n#&gt; During these first days of his life his\n#&gt; home was in the heart of a great windfall\n#&gt; where aray wolf, his blind mother, had found\n#&gt; a a safe nest for his hahy hood, and to which\n#&gt; Kazan, her mate, came only now and then ,\n#&gt; his eyes gleaming like strange balls of greenish\n#&gt; fire in the darknen. It was kazan's eyes that\n#&gt; gave\n#&gt; do Barce his first impression of something\n#&gt; existing away from his mother's side, and they\n#&gt; brought to him also his discovery of vision. He\n#&gt; could feel, he could smell, he could hear - but\n#&gt; in that black pirt under the fallen timher he\n#&gt; had never seen until the\n#&gt; eyes\n#&gt; came. At first\n#&gt; they frightened nin; then they puzzled him , and\n#&gt; bis Heer changed to an immense ceniosity, the world\n#&gt; be looking foreight at them when all at once\n#&gt; they world disappear. This was when Kazan turned\n#&gt; his head. And then they would flash hach at him\n#&gt; again wt of the darknen with such startling\n#&gt; Suddenness that Baree world involuntanty Shrink\n#&gt; closer to his mother who always treunded and\n#&gt; Shivered in a strenge way when Kazan came in.\n#&gt; Barce, of course, would never know their story. He\n#&gt; world never know that Gray Wolf, his mother, was\n#&gt; a full-hlooded wolf, and that Kazan, his father,\n#&gt; was a dog. In hin nature was already\n#&gt; nature was already beginning\n#&gt; its wonderful work, but it world never go beyind\n#&gt; cerria limitations. It wald tell him, in time, ,\n#&gt; that his heavtiful wolf - mother was blind, hur\n#&gt; he world never know of that terrible hattle between\n#&gt; Gray Wolf and the lynx in which his mother's sight\n#&gt; had been destroyed Nature could tell hin gatting\n#&gt; nothing\n#&gt; +\n#&gt; а\n#&gt; (\nA lot of the text is readable but there is a lot to do to fix all of the errors. And this is only one page! Maybe things would be better for someone with better handwriting than me.\nIn the next section I’ll try to quantify how close the text extracted by Vision API from the handwritten pages is to the original text by measuring edit distance to the equivalent page of text from Project Gutenberg."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#how-well-does-it-work",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#how-well-does-it-work",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "How well does it work?",
    "text": "How well does it work?\nTo download the original text from Project Gutenberg use the {gutenbergr} (Robinson (2020)) package.\nbaree &lt;- gutenbergr::gutenberg_download(4748)\nIf you get an error here, try a different mirror.\nFrom the text of the entire novel we can extract just the portion that corresponds to our scanned pages. In this case that means lines 96 to 148.\nbaree_tx &lt;- paste(baree$text[96:148], collapse = \" \")\nWith a bit more close inspection we can find the substring of the downloaded text corresponding to the first handwritten page. In this case the substring beginning at the 1st character and ending at the 1597th.\ncat(baree_tx_001 &lt;- stringr::str_sub(baree_tx, 1, 1597))\n#&gt; To Baree, for many days after he was born, the world was a vast gloomy cavern.  During these first days of his life his home was in the heart of a great windfall where Gray Wolf, his blind mother, had found a safe nest for his babyhood, and to which Kazan, her mate, came only now and then, his eyes gleaming like strange balls of greenish fire in the darkness. It was Kazan's eyes that gave to Baree his first impression of something existing away from his mother's side, and they brought to him also his discovery of vision. He could feel, he could smell, he could hear--but in that black pit under the fallen timber he had never seen until the eyes came. At first they frightened him; then they puzzled him, and his fear changed to an immense curiosity. He would be looking straight at them, when all at once they would disappear. This was when Kazan turned his head. And then they would flash back at him again out of the darkness with such startling suddenness that Baree would involuntarily shrink closer to his mother, who always trembled and shivered in a strange sort of way when Kazan came in.  Baree, of course, would never know their story. He would never know that Gray Wolf, his mother, was a full-blooded wolf, and that Kazan, his father, was a dog. In him nature was already beginning its wonderful work, but it would never go beyond certain limitations. It would tell him, in time, that his beautiful wolf mother was blind, but he would never know of that terrible battle between Gray Wolf and the lynx in which his mother's sight had been destroyed. Nature could tell him nothing\nNow using the {stringdist} (der Loo (2014)) package calculate the edit distance between the text extracted from the handwritten page by Vision API and the string extracted from the Project Gutenberg text.\nstringdist::stringdist(baree_hw_001, baree_tx_001, method = \"lv\")\n#&gt; [1] 174\nApparently we could change the handwriting based text to match the text from Project Gutenberg with 174 changes. Quite a lot for one page! However, bear in mind that edit distance is not quite the same as the number of changes you need to make to fix the document yourself. You could use spell check tools to fix some errors quickly and use search-and-replace to remove systematic errors like non-alphabetic characters (assuming your text is entirely alphabetic) or additional spacing.\nIn calculating edit distance we used the default optimal string alignment method from {stringdist}. But the result is the same if we use different methods like Levenshtein distance (method = \"lv\") or Full Damerau-Levenshtein distance (method = \"dl\")."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#building-a-request-based-on-multiple-images",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#building-a-request-based-on-multiple-images",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "Building a request based on multiple images",
    "text": "Building a request based on multiple images\nBegin by putting images of all pages to be converted into the same folder. Here I’ve used the folder ~/workspace/baree-handwriting-scans.\nscans_folder &lt;- \"~/workspace/baree-handwriting-scans\"\nNow use list.files with full.names = TRUE and pattern = '*.png' (assuming your images are PNG format) to get a list of all images.\nscans &lt;- list.files(scans_folder, pattern = '*.png', full.names = TRUE)\nNext, iterate over scans with purrr::map and document_text_detection to create a list of JSON request objects, one for each page.\nscans_dtd &lt;- purrr::map(scans, document_text_detection)\nAs before, wrap this list of requests in another list.\nl_r &lt;- list(requests = scans_dtd)\nBefore calling post_vision and sending all images to Vision API.\nresponse &lt;- post_vision(l_r)\nNotice that even though there are multiple images we still only make one request. The JSON payload contains encodings of all images.\nDepending on the number of images in the requests list the above call to post_vision may take a few seconds to return a response.\nAs before, check that a valid response was received before opening it up and looking at what is inside.\nhttr::status_code(response)\n#&gt; [1] 200\nIf the response is valid you can iterate through the responses list inside the response content extracting fullTextAnnotation from each element.\nThis can be done with purrr::map by passing the name \"fullTextAnnotation\" as the function argument.\nresponses_annotations &lt;- purrr::map(httr::content(response)$responses, \"fullTextAnnotation\")\npurrr::map converts \"fullTextAnnotation\" into an extractor function which pulls out elements of the list argument having that name.\nUsing the same feature of purrr::map you can reach inside the annotations and pull out any text elements. This time using purrr::map_chr instead of purrr:map because the output should be a string.\npurrr::map_chr(responses_annotations, \"text\")\n#&gt; [1] \"a\\nvast\\nfear\\nTo Parce, for many days after he was\\nborn, the world was a\\ngloony cavern.\\nDuring these first days of his life his\\nhome was in the heart of a great windfall\\nwhere aray wolf, his blind mother, had found\\na a safe nest for his hahy hood, and to which\\nKazan, her mate, came only now and then ,\\nhis eyes gleaming like strange balls of greenish\\nfire in the darknen. It was kazan's eyes that\\ngave\\ndo Barce his first impression of something\\nexisting away from his mother's side, and they\\nbrought to him also his discovery of vision. He\\ncould feel, he could smell, he could hear - but\\nin that black pirt under the fallen timher he\\nhad never seen until the\\neyes\\ncame. At first\\nthey frightened nin; then they puzzled him , and\\nbis Heer changed to an immense ceniosity, the world\\nbe looking foreight at them when all at once\\nthey world disappear. This was when Kazan turned\\nhis head. And then they would flash hach at him\\nagain wt of the darknen with such startling\\nSuddenness that Baree world involuntanty Shrink\\ncloser to his mother who always treunded and\\nShivered in a strenge way when Kazan came in.\\nBarce, of course, would never know their story. He\\nworld never know that Gray Wolf, his mother, was\\na full-hlooded wolf, and that Kazan, his father,\\nwas a dog. In hin nature was already\\nnature was already beginning\\nits wonderful work, but it world never go beyind\\ncerria limitations. It wald tell him, in time, ,\\nthat his heavtiful wolf - mother was blind, hur\\nhe world never know of that terrible hattle between\\nGray Wolf and the lynx in which his mother's sight\\nhad been destroyed Nature could tell hin gatting\\nnothing\\n+\\nа\\n(\\n\"\n#&gt; [2] \"7\\n9\\n49\\n7\\nof Kazan's merciless vengeance 1 of the wonderful\\nyears of their matehood of their loyalty, their\\nShenge adventures in the great Canadian wilderness\\ncit'culd make him arby a son of hazar.\\nBut at first, and for many days, it was all\\nMother. Even after his eyes opened wide and he\\nhad pund his legs so that he could shonhce around\\na little in the darkness, nothing existed ar buree\\nfor\\nhut his mother. When he was old enough to he\\n.\\nplaying with Shicks and mess art in the sunlight,\\nhe still did not know what she looked like. But\\nto him she was big and soft and warm, and she\\nhicked his face with her tongue, and talked to him\\nin a gentle, whimpening way that at lost made\\nhim find his own voice in a faint, squeaky yap.\\nAnd then came that wonderful day when the\\ngreenish balls of fire that were kažan's eyes cancie\\nnearer and nearer, a little at a tine, ,\\ncarbiesky. Hereto pore Gray Wolf had warned hin\\nhach. To he alone was the first law of her wild\\nbreed during mothering time. A low snart from her\\n. A\\nthroat, ånd Kazan' had always stopped. But\\nănd\\non this day the snart did not come in aray\\nWolf's throat it died away in a low, whimpering\\nscond. A note of loneliness, of\\ni 아\\ngreat yearniny _“It's all night law,\\\" she was\\nť keys, of a\\nnow\\nsaying to kázan; and katan\\npowsing for a moment\\nreplied with an answłni\\nwswering\\ndeep in his throat.\\nStill slowly, as it not quite sure of what he\\nЕ\\nwould find, Kazan came to them, and Baree\\nsnuggled closer to his mother\\nas he dropped down heavily on his belly close to\\naray Wolf. He was unafraid\\nand nightily\\nand\\nvery\\n.\\nC\\nto make ure -\\nnote\\nHe heard kazan\\n\"\nThe folder_to_chr function below puts all the above steps together.\nInput to folder_to_chr is a path to a folder of images whose text we want to extract. The return value is list of strings containing all text contained in those images.\nfolder_to_chr &lt;- function(scans_folder) {\n  scans &lt;- list.files(scans_folder, pattern = \"*.png\", full.names = TRUE)\n  response &lt;- post_vision(list(requests = purrr::map(scans, document_text_detection)))\n  responses_annotations &lt;- purrr::map(httr::content(response)$responses, \"fullTextAnnotation\")\n  purrr::map_chr(responses_annotations, \"text\")\n}\nFor example, to convert all the pages in scans_folder and combine the results into a single string do something like this:\nbaree_hw &lt;- paste(folder_to_chr(scans_folder), collapse = \" \")"
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#how-well-does-it-work-1",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#how-well-does-it-work-1",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "How well does it work?",
    "text": "How well does it work?\nAs before you can use the edit distance between baree_hw and the equivalent pages of the original text. Here, that text has been extracted before into baree_tx.\nstringdist::stringdist(baree_hw, baree_tx)\n#&gt; [1] 446\nIs this high? It’s hard to know without something to compare against.\nFor example, how does it compare to the edit distance between a random string of the same length as the target text and the target text itself?\nrandom_text &lt;- paste(sample(c(letters, \" \"), stringr::str_length(baree_tx), replace = TRUE), collapse = \"\")\nstringdist::stringdist(random_text, baree_tx)\n#&gt; [1] 2882\nIt’s hardly surprising that this is a much higher number.\nAnother comparison we could make is against the result of using Tesseract to extract text from the same handwritten pages.\nFortunately, the {tesseract} (Ooms (2021)) package makes this very easy.\nfolder_to_chr_ts &lt;- function(scans_folder) {\n  eng &lt;- tesseract::tesseract(\"eng\")\n  scans &lt;- list.files(scans_folder, pattern = \"*.png\", full.names = TRUE)\n  paste(purrr::map(scans, tesseract::ocr, engine = eng), collapse = \" \")\n}\n\nbaree_hw_ts &lt;- folder_to_chr_ts(scans_folder)\nstringdist::stringdist(baree_hw_ts, baree_tx)\n#&gt; [1] 1502\nSo Vision API does much better than Tesseract according to this particular little test. But this is probably not a fair comparison. Tesseract doesn’t claim to be able to extract text from images of handwriting. Furthermore, it may be possible to configure Tesseract in a way that improves the results or to modify the input images to make them better suited to Tesseract. Also, I’m only looking at my own handwriting. It might be that another person’s handwriting works better with Tesseract than mine.\nMy objective was just to see if using Vision API made it possible to convert my own handwritten documents into text. While promising it seems that I might spend longer fixing errors in the resulting text than it would take to type them out from scratch."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#install-the-cloud-sdk",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#install-the-cloud-sdk",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "0. Install the cloud SDK",
    "text": "0. Install the cloud SDK\nYou need to install the glcoud tool to configure authentication.\nThe installation instructions are here: https://cloud.google.com/sdk/docs/install"
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#create-project",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#create-project",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "1. Create project",
    "text": "1. Create project\nFirst you need to create a GCP project and enable Vision API for that project.\nYou will have to setup billing when creating the project if you haven’t done so before.\n\nGo to the Project Selector: https://console.cloud.google.com/projectselector2/home/dashboard and select Create Project\nGive your project a name and click Create.\nEventually the Project Dashboard appears. At the time of writing somewhere in the bottom-left of the dashboard is a Getting Started panel containing a link named Explore and enable APIs. Click on it.\nYou will be transported to the API dashboard.\nAt the top you should see + ENABLE APIs AND SERVICES. Click on that.\nNow you get a search dialog.\nType vision and press enter.\nSelect Cloud Vision API and on the page that appears click the Enable button."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#create-service-account",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#create-service-account",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "2. Create service account",
    "text": "2. Create service account\nThen you need to create a service account.\nThis is a good point to just give up.\nStrictly speaking this isn’t necessary but it does seem to be the most straightforward way of enable authentication for your project.\n\nClick the hamburger in the navigation bar to open the sidebar menu.\nScroll down to IAM & Admin and select Service Accounts.\nClick on Create Service Account.\nGive your account a name.\nClick Create and continue"
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#download-private-key",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#download-private-key",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "3. Download private key",
    "text": "3. Download private key\n\nIn the Service accounts dashboard find the service account you created and click on the three dots below Action.\nSelect Manage Keys from the drop-down menu.\nOn the page that open click on the ADD KEY button.\nChoose Create New Key from the drop-down menu.\nClick Create on the modal dialog that opens.\nYou will be prompted to save your key.\nDownload your private key and remember where you save it (the save location is referenced below as &lt;PATH_TO_PRIVATE_KEY&gt;)."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#use-gcloud-tool-to-get-access-token",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#use-gcloud-tool-to-get-access-token",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "4. Use gcloud tool to get access token",
    "text": "4. Use gcloud tool to get access token\nNow you can use gcloud to get the access token required in the above tutorial.\nFirst, define an environment variable GOOGLE_APPLICATION_CREDENTIALS pointing to the location where you saved the private key.\n$ export GOOGLE_APPLICATION_CREDENTIALS=&lt;PATH_TO_PRIVATE_KEY&gt;\nThen run\n$ gcloud auth application-default print-access-token\nThe output will be your access token followed by lots of dots."
  },
  {
    "objectID": "posts/2023-05-04-generating-examples-of-maximal-room-squares-in-r/index.html",
    "href": "posts/2023-05-04-generating-examples-of-maximal-room-squares-in-r/index.html",
    "title": "Generating Examples of Maximal Room Squares in R",
    "section": "",
    "text": "A Room square of order \\(n\\) and side \\(n − 1\\) on an \\(n\\)‐element set \\(S\\) is an \\(n - 1 \\times n - 1\\) array filled with \\(n\\) different symbols in such a way that:\nA partial Room square of order \\(n\\) and side \\(n − 1\\) on an \\(n\\)‐element set \\(S\\) is an \\(n - 1 \\times n - 1\\) array satisfying property (1) above, and also\nA partial Room square is maximal if no further pair of elements of \\(S\\) can be placed into any unoccupied cell of \\(F\\) without violating the conditions (1), (4), (5)."
  },
  {
    "objectID": "posts/2023-05-04-generating-examples-of-maximal-room-squares-in-r/index.html#greedy1",
    "href": "posts/2023-05-04-generating-examples-of-maximal-room-squares-in-r/index.html#greedy1",
    "title": "Generating Examples of Maximal Room Squares in R",
    "section": "greedy1",
    "text": "greedy1\nThe algorithm greedy1 visits each cell in a predetermined order and places the first available pair of symbols into the cell, provided that doing so does not violate the conditions of creating a partial Room square.\nR &lt;- greedy1(6)\nplot_room_square_labs(R)\n\nIn this plot, the colors indicate the sequence in which the cells were filled. Specifically, lighter colors represent cells that were filled earlier in the process, while darker colors represent cells that were filled later.\nis_maximal_proom(R)\n#&gt; [1] TRUE\nHere are a few more examples of maximal partial Room squares created by greedy1."
  },
  {
    "objectID": "posts/2023-05-04-generating-examples-of-maximal-room-squares-in-r/index.html#greedy2",
    "href": "posts/2023-05-04-generating-examples-of-maximal-room-squares-in-r/index.html#greedy2",
    "title": "Generating Examples of Maximal Room Squares in R",
    "section": "greedy2",
    "text": "greedy2\nThe algorithm greedy2 iterates through all pairs of symbols in a predetermined order and places the next available pair into the first empty cell, provided that doing so does not violate the conditions of creating a partial Room square.\nR &lt;- greedy2(6)\nplot_room_square_labs(R)\n\nis_maximal_proom(R)\n#&gt; [1] TRUE\nHere are a few more examples of maximal partial Room squares created by greedy2."
  },
  {
    "objectID": "posts/2019-12-17-tidy-tuesday-adoptable-dogs/index.html",
    "href": "posts/2019-12-17-tidy-tuesday-adoptable-dogs/index.html",
    "title": "Tidy Tuesday: Adoptable Dogs",
    "section": "",
    "text": "Tidy Tuesday for week fifty-one of 2019 was all about dogs available for adoption in the United States through PetFinder.\nThe data was collected by Amber Thomas for her piece Finding Forever Homes which appeared in The Pudding in October 2019.\nThere were three data sets. One data set about dogs that were moved between states for adoption. Another described where dogs were orginally found. The third gave detailed information about every dog.\nI created two plots from this data.\nThe first plot shows how imports and exports vary by state. I used Bob Rudis’s {statebins} package Rudis (2020) for this plot as well as Thomas Lin Pedersen’s {patchwork} package Pedersen (2020).\nYou can see that lots of dogs are exported from southern states, particularly Texas. You can also see that lot of dogs are imported to northern states like Washington and New York.\nThe second plot shows the one hundred most popular dog names among all dogs available for adoption.\nThe colour of a name in this plot indicates whether it has one, two or three syllables. I used Tyler Rinker’s {syllable} package Rinker (2017) to do this."
  },
  {
    "objectID": "posts/2023-05-16-tidy-tuesday-tornadoes/index.html",
    "href": "posts/2023-05-16-tidy-tuesday-tornadoes/index.html",
    "title": "Tidy Tuesday: Tornadoes",
    "section": "",
    "text": "This week Tidy Tuesday was all about tornadoes. Data came from NOAA’s National Weather Service Storm Prediction Center Severe Weather Maps, Graphics, and Data Page.\nTo draw the maps below I used the {{sf}} package in R. I used geom_sf to plot both the Kentucky state boundary using data from the {{tigris}} package as well as tornado paths from the NOAA data.\nThe first version I created was too clutterred and hard to interpret. So I tried again, introducing a decade variable so that I could plot different tornadoes from different decades on different maps with geom_facet.\n\n\n\nThis image shows a grid of maps of the US state of Kentucky. Each map represents a different decade and is filled with coloured arrows showing the paths of tornadoes in that decade. The arrows are coloured according to the intensity of the tornado. The plot shows that during the 1980s there were relatively few tornadoes in Kentucky while in the 1970s there were a large number of very intense torndoes. In recent decades the number of tornadoes appears to have increased but there are fewer of high intensity.\n\n\nThe final version of this plot originally included county boundaries. But I decided to remove them as they interfered with the state boundary and made the plot too messy when combined with lots of arrows. I wish I could have found a way to add the county boundaries in a more subtle way so they could be seen but without affecting the overall simplicity of the final plot.\nAlthough the final plot arguably lacks a compelling story I think it does have the property of allowing the viewer to find something interesting by exploring the plot.\nFor example, after looking at the plot for a while it became clear to me that during the 1970s there were a large number of powerful tornadoes in Kentucky, particularly in the central region of the state and especially when compared to the 1980s. In more recent decades it seems that the western part of the state has experienced most tornado activity.\nSpending some time on titles, lables and headings improved the plot quite a bit. I took a little bit of extra care labelling the 1950s and 2020s facets properly as well as finding space to describe the change of scale in 2007 from F to EF. I used one my favourite themes from the {{hrbrthemes}} package which always improves the overall look of the final result.\nThis was the first time I used Github Codespaces for an entire project (albeit a very small project). All of the work I did on this plot was done on Codespaces with the rocker/geospatial Docker container.\nCombining {{renv}} and {{targets}} made things especially easy. Resuming work inside a fresh Codespace is just a matter of calling renv::restore() followed by targets::tar_make(). Package installations are fast thanks to RStudio Package Manager."
  }
]