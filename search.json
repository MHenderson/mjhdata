[
  {
    "objectID": "posts/2014-10-17-euler-paths/index.html",
    "href": "posts/2014-10-17-euler-paths/index.html",
    "title": "Euler Paths",
    "section": "",
    "text": "One of the oldest problems in graph theory concerns the Eastern Prussia old city of Königsberg. In that city was an island around which the river Pregel flowed in two branches. Seven bridges crossed the river to enable people to cross between mainland and island. A drawing of the river, bridges and island is shown below.\nAt some point the question of whether it was possible to devise a tour of the city to cross every one of the bridges once and once only was raised. In 1736, Euler showed that no such route is possible.\nEuler explained his solution in Euler (1741) which has been translated in the first chapter of Biggs, Lloyd, and Wilson (1986) . The original version “Solutio Problematis Ad Geometriam Situs Pertinentis” is available for download from the MAA Euler Archive.\nThe goal of this post is to reproduce some ideas from Euler’s paper in computer language, specifically for the Maxima computer algebra system. We describe an implementation of multigraphs in Maxima and an approach to deciding whether a path in a multigraph is an Euler path or not. In a future post we will revisit this topic and discuss searching multigraphs for Euler paths."
  },
  {
    "objectID": "posts/2014-10-17-euler-paths/index.html#multigraphs-in-maxima",
    "href": "posts/2014-10-17-euler-paths/index.html#multigraphs-in-maxima",
    "title": "Euler Paths",
    "section": "Multigraphs in Maxima",
    "text": "Multigraphs in Maxima\nMaxima comes with a module for working with graphs. Unfortunately, the Maxima graphs module requires graphs to be simple, having no parallel edges and no loops. The graph which arises in the Königsberg bridges problem, however, is not simple.\nOne way to represent a multigraph is as a pair \\(G = (V, E)\\) where \\(V\\) is a set of vertices and \\(E\\) is a set of edges. An edge ,in this context, is a pair \\((e, \\{u, v\\})\\) where \\(e\\) is an edge-label and \\(u, v\\) are the end-vertices of \\(e\\). This representation allows edges to have the same ends and only to differ in label. Loops in this setting would be pairs of the form \\((e, u)\\) or \\((e, \\{u\\})\\). As none of the graphs we consider here contain loops we ignore the added complications of allowing loops.\nMaxima makes it easy to create new simple data structures. The defstruct function adds a new structure to Maxima’s list of user-defined structures. A structure in Maxima has the same syntax as a function signature. The function name becomes the name of a new structure and its arguments become fields of structures of defined with new and can then be accessed with the @ syntax.\nFor example, we can define a graph structure like so:\n(%i) defstruct(graph(V, E))$\nThen a multigraph representing the bridges of Königsberg can be created like so:\n(%i) konigsberg: new(graph({A,B,C,D},\n                          {[a,{A,B}],\n                           [b,{A,B}],\n                           [c,{A,C}],\n                           [d,{A,C}],\n                           [e,{A,D}],\n                           [f,{B,D}],\n                           [g,{C,D}]}))$\nThe vertices of this multigraph are the regions of land, either mainland or island:\n(%i) konigsberg@V;\n(%o)                           {A, B, C, D}\nThe edges are bridges. The label of an edge being the label of the bridge in Euler’s diagram and the ends are the vertices (regions of land) joined by the bridge in question:\n(%i) konigsberg@E;\n(%o) [[a, {A, B}], [b, {A, B}], [c, {A, C}], [d, {A, C}],\n                    [e, {A, D}], [f, {B, D}], [g, {C, D}]]\nTo access to the ends of a specific edge use the assoc function which gives a list or set of pairs the interface of an associative structure:\n(%i) assoc(a, konigsberg@E);\n(%o)                               {A, B}"
  },
  {
    "objectID": "posts/2014-10-17-euler-paths/index.html#euler-paths-in-maxima",
    "href": "posts/2014-10-17-euler-paths/index.html#euler-paths-in-maxima",
    "title": "Euler Paths",
    "section": "Euler Paths in Maxima",
    "text": "Euler Paths in Maxima\nA path in Biggs, Lloyd, and Wilson (1986) is defined as a sequence of vertices and edges \\(v_{0},e_{1},v_{1},e_{2},v_{2},\\ldots,v_{r-1},e_{r},v_{r}\\) in which each edge \\(e_{i}\\) joins vertices \\(v_{i-1}\\) and \\(v_{i}\\) \\((1\\leq i\\leq r)\\). An Euler path is a path for which \\(r = |E|\\), where \\(|E|\\) is the size of the multigraph.\nIn Maxima paths (and non-paths) can be represented by lists of symbols. To distinguish those lists of symbols which truly represent a path in a graph we will have to check the defining properties of a path. Namely we have to be sure that\n\nevery \\(v_{i}\\) is a vertex of \\(G\\),\nevery \\(e_{i}\\) is a edge of \\(G\\),\nevery \\(e_{i}\\) is an edge of \\(G\\) which joins vertices \\(v_{i-1}\\) and \\(v_{i}\\) of \\(G\\).\n\nAs the third condition subsumes the other two and as we are only concerned with correctness here and not, yet, efficiency we can ignore the first two conditions and only check the third one.\nSo if \\(P\\) is a list of symbols then \\(P\\) is a path of multigraph \\(G\\) if and only if\n{P[i-1], P[i+1]} = assoc(P[i], G@E))\nholds for all i from 2 to (length(P) - 1)/2 [lists in Maxima being indexed from 1]. This condition expresses the fact that symbols adjacent to the ith symbol are the ends of the edge represented by that symbol in some order. Notice that this condition requires that the list has the vertex-edge-vertex structure of a path.\nNow we can define a function path(G, P) that decides whether \\(P\\) is a path in \\(G\\) or not:\npath(G, P):= block(\n [result: true],\n for i: 2 step 2 thru (length(P)-1) do (\n   result: result and is({P[i-1], P[i+1]} = assoc(P[i], G@E))\n ),\n return(result)\n)$\nWith this function available, testing for Euler paths is only a matter of testing whether a path has length equal 2*length(G@E) + 1:\neuler_path(G, P):= (\n is(path(G, P)) and is(length(P) = 2*length(G@E) + 1)\n)$\nAs a test of this function we check that an example of an Euler path in Euler (1741) really is an Euler path. As the bridges of Königsberg multigraph has on Euler path, Euler considers a fictitious map, shown below:\n\n\n\nConnected Graphs\n\n\nHe claims that \\(EaFbBcFdAeFfCgAhCiDkAmEnApBoElD\\) is an Euler path in this map. We can check by hand but now we can also represent the multigraph in Maxima and check using the above implementation of euler_path:\n(%i) eulersberg: new(graph({A,B,C,D,E,F},\n                          {[a,{E,F}],\n                           [b,{B,F}],\n                           [c,{B,F}],\n                           [d,{A,F}],\n                           [e,{A,F}],\n                           [f,{C,F}],\n                           [g,{A,C}],\n                           [h,{A,C}],\n                           [i,{C,D}],\n                           [k,{A,D}],\n                           [l,{D,E}],\n                           [m,{A,E}],\n                           [n,{A,E}],\n                           [o,{B,E}],\n                           [p,{A,B}]}))$\n(%i) s: \"EaFbBcFdAeFfCgAhCiDkAmEnApBoElD\"\n(%i) journey: map(eval_string, charlist(s))$\n(%i) euler_path(eulersberg, journey);\n(%o)                                true"
  },
  {
    "objectID": "posts/2023-05-16-tidy-tuesday-tornadoes/index.html",
    "href": "posts/2023-05-16-tidy-tuesday-tornadoes/index.html",
    "title": "Tidy Tuesday: Tornadoes",
    "section": "",
    "text": "This week Tidy Tuesday was all about tornadoes. Data came from NOAA’s National Weather Service Storm Prediction Center Severe Weather Maps, Graphics, and Data Page.\nTo draw the maps below I used the {{sf}} package in R. I used geom_sf to plot both the Kentucky state boundary using data from the {{tigris}} package as well as tornado paths from the NOAA data.\nThe first version I created was too clutterred and hard to interpret. So I tried again, introducing a decade variable so that I could plot different tornadoes from different decades on different maps with geom_facet.\n\n\n\nThis image shows a grid of maps of the US state of Kentucky. Each map represents a different decade and is filled with coloured arrows showing the paths of tornadoes in that decade. The arrows are coloured according to the intensity of the tornado. The plot shows that during the 1980s there were relatively few tornadoes in Kentucky while in the 1970s there were a large number of very intense torndoes. In recent decades the number of tornadoes appears to have increased but there are fewer of high intensity.\n\n\nThe final version of this plot originally included county boundaries. But I decided to remove them as they interfered with the state boundary and made the plot too messy when combined with lots of arrows. I wish I could have found a way to add the county boundaries in a more subtle way so they could be seen but without affecting the overall simplicity of the final plot.\nAlthough the final plot arguably lacks a compelling story I think it does have the property of allowing the viewer to find something interesting by exploring the plot.\nFor example, after looking at the plot for a while it became clear to me that during the 1970s there were a large number of powerful tornadoes in Kentucky, particularly in the central region of the state and especially when compared to the 1980s. In more recent decades it seems that the western part of the state has experienced most tornado activity.\nSpending some time on titles, lables and headings improved the plot quite a bit. I took a little bit of extra care labelling the 1950s and 2020s facets properly as well as finding space to describe the change of scale in 2007 from F to EF. I used one my favourite themes from the {{hrbrthemes}} package which always improves the overall look of the final result.\nThis was the first time I used Github Codespaces for an entire project (albeit a very small project). All of the work I did on this plot was done on Codespaces with the rocker/geospatial Docker container.\nCombining {{renv}} and {{targets}} made things especially easy. Resuming work inside a fresh Codespace is just a matter of calling renv::restore() followed by targets::tar_make(). Package installations are fast thanks to RStudio Package Manager."
  },
  {
    "objectID": "posts/2019-12-17-tidy-tuesday-adoptable-dogs/index.html",
    "href": "posts/2019-12-17-tidy-tuesday-adoptable-dogs/index.html",
    "title": "Tidy Tuesday: Adoptable Dogs",
    "section": "",
    "text": "Tidy Tuesday for week fifty-one of 2019 was all about dogs available for adoption in the United States through PetFinder.\nThe data was collected by Amber Thomas for her piece Finding Forever Homes which appeared in The Pudding in October 2019.\nThere were three data sets. One data set about dogs that were moved between states for adoption. Another described where dogs were orginally found. The third gave detailed information about every dog.\nI created two plots from this data.\nThe first plot shows how imports and exports vary by state. I used Bob Rudis’s {statebins} package Rudis (2020) for this plot as well as Thomas Lin Pedersen’s {patchwork} package Pedersen (2020).\nYou can see that lots of dogs are exported from southern states, particularly Texas. You can also see that lot of dogs are imported to northern states like Washington and New York.\nThe second plot shows the one hundred most popular dog names among all dogs available for adoption.\nThe colour of a name in this plot indicates whether it has one, two or three syllables. I used Tyler Rinker’s {syllable} package Rinker (2017) to do this."
  },
  {
    "objectID": "posts/2021-05-05-tidy-tuesday-water-sources/index.html",
    "href": "posts/2021-05-05-tidy-tuesday-water-sources/index.html",
    "title": "Tidy Tuesday: Water Sources",
    "section": "",
    "text": "This week’s Tidy Tuesday data comes from Water Point Data Exchange (WPDx), a global platform for sharing water point data.\nInspired by David Robinson’s livestream on 4/5/21 I created a faceted map plot showing the locations of different water sources throughout Ethiopia.\n{{% youtube “5ub92c-5xFQ” %}}\nThis was my first time using Thomas Lin Pedersen’s {ragg} package. It allowed me to use one of my favourite fonts, Cardo.\nI was also inspired by the work of Georgios Karamanis and a tweet by Nicola Rennie.\nSource code: https://github.com/MHenderson/water-sources"
  },
  {
    "objectID": "posts/2023-05-09-tidy-tuesday-childcare-costs/index.html",
    "href": "posts/2023-05-09-tidy-tuesday-childcare-costs/index.html",
    "title": "Tidy Tuesday: Childcare Costs",
    "section": "",
    "text": "The plot below is my attempt to recreate in {{ggplot2}} one of the plots at https://www.dol.gov/agencies/wb/topics/childcare/price-by-age-care-setting. Namely the plot that greets you when you follow the above link, with the price selector dropdown at the default value of “Infant center-based”.\n\n\n\nA chloropleth map showing the median cost of childcare in the USA between 2016 and 2018. The coloured regions of the map correspond to counties.\n\n\nI faced one or two difficulties in trying to recreate the original plot. One issue I didn’t resolve was how to adjust the prices for CPI-U (the Consumer Price Index for All Urban Consumers). It was easy enough to figure out what calculation needed to be done but I couldn’t find a source of CPI-U for childcare costs in the USA between 2016 and 2018.\nAnother issue I failed to resolve was how to add maps of Alaska and Hawaii to the plot of the rest of the USA.\nThe code behind my plot is here: https://github.com/MHenderson/childcare-costs"
  },
  {
    "objectID": "posts/2014-09-26-greedy-edge-colouring-of-small-graphs/index.html",
    "href": "posts/2014-09-26-greedy-edge-colouring-of-small-graphs/index.html",
    "title": "Greedy Edge Colouring of Small Graphs",
    "section": "",
    "text": "In seveal earlier posts we looked at greedy vertex-colouring of small graphs. As we saw, a greedy approach to vertex-colouring is quite successful in so far as it uses at most \\(\\Delta(G) + 1\\) colours to colour any graph \\(G\\).\nIt is easy to modify the greedy method to colour the edges of a graph. However, we cannot guarantee that the number of colours used will be as few as \\(\\Delta(G) + 1\\). The best that we can guarantee with the simplest greedy approach to edge-colouring is no more than \\(2\\Delta(G) - 1\\) colours.\nIt’s not difficult to see why this is, for suppose that we have coloured some edges of the graph and come to colour edge \\(e = uv\\). There might be as many as \\(\\Delta(G) - 1\\) colours on edges incident with \\(u\\) and the same amount on edges incident with \\(v\\). In the worst case, all of these \\(2\\Delta(G) - 2\\) colours might be different and so we need at least \\(2\\Delta(G) - 1\\) colours in our palette to be certain, without recolouring, to have a colour available for edge \\(e\\).\nIn this post we introduce a NetworkX-based implementation of greedy edge-colouring for graphs in graph6 format. Using this implementation we investigate the average case performance on all non-isomorphic, connected simple graphs of at most nine vertices. It turns out that, on average, the greedy edge-colouring method uses many fewer colours than the worst case of \\(2\\Delta(G) - 1\\).\nAs we will discuss, the theory of edge-colouring suggests that with large sets of simple graphs we can get close, on average, to the best case of \\(\\Delta(G)\\) colours."
  },
  {
    "objectID": "posts/2014-09-26-greedy-edge-colouring-of-small-graphs/index.html#greedy-edge-colouring-with-networkx",
    "href": "posts/2014-09-26-greedy-edge-colouring-of-small-graphs/index.html#greedy-edge-colouring-with-networkx",
    "title": "Greedy Edge Colouring of Small Graphs",
    "section": "Greedy Edge-Colouring with NetworkX",
    "text": "Greedy Edge-Colouring with NetworkX\nThe core of our implementation is a function that visits every edge of a graph and assigns a colour to each edge according to a parametrised colour choice strategy.\ndef edge_colouring(G, choice = choice_greedy):\n    max_degree = max(G.degree().values())\n    palette = range(0, 2*max_degree)\n    for e in G.edges():\n        colour_edge(G, e, choice(G, e, palette))\nThis function allows for some flexibility in the method used to choose the colour assigned to a certain edge. Of course, it lacks flexibility in certain other respects. For example, both the order in which edges are visited and the palette of colours are fixed.\nEverything in the implementation is either Python or NetworkX, except for the colour_edge(G, e, c) and choice(G, e, p) functions. The former simply applies colour c to edge e in graph G. The latter, a function parameter that can be specified to implement different colouring strategies, decides the colour to be used.\nFor greedy colouring the choice strategy is plain enough. For edge \\(e = uv\\) in graph \\(G\\) we choose the first colour from a palette of colours which is not used on edges incident with either vertex \\(u\\) or vertex \\(v\\). The implementation, below, is made especially simple by Python’s Sets.\ndef choice_greedy(G, e, palette):\n    used_colours = used_at(G, e[0]).union(used_at(G, e[1]))\n    available_colours = set(palette).difference(used_colours)\n    return available_colours.pop()\nHere used_at(G, u) is a function that returns a Set of all colours used on edges incident with u in G. So, via the union operation on Sets, used_colours becomes the set of colours used on edges incident with end-vertices of e. The returned colours is then the colour on the top of available_colours, the set difference of palette and used_colours."
  },
  {
    "objectID": "posts/2014-09-26-greedy-edge-colouring-of-small-graphs/index.html#edge-colouring-small-graphs",
    "href": "posts/2014-09-26-greedy-edge-colouring-of-small-graphs/index.html#edge-colouring-small-graphs",
    "title": "Greedy Edge Colouring of Small Graphs",
    "section": "Edge-Colouring Small Graphs",
    "text": "Edge-Colouring Small Graphs\nThe implementation described in the previous section has been put into a script that processes graphs in graph6 format and returns, not the edge-colouring, but the number of colours used. For example, the number of colours used in a greedy edge-colouring of the Petersen graph is four:\n$ echo ICOf@pSb? | edge_colouring.py\n4\nAs in earlier posts on vertex-colouring we now consider the set of all non-isomorphic, connected, simple graphs and study the average case performance of our colouring method on this set. For vertex-colouring, parallel edges have no effect on the chromatic number and thus the set of simple graphs is the right set of graphs to consider. For edge-colouring we ought to look at parallel edges and thus the set of multigraphs because parallel edges can effect the chromatic index. We will save this case for a future post.\nAlso in common with earlier posts, here we will use Drake as the basis for our simulation. The hope being that others can reproduce our results by downloading our Drakefile and running it.\nWe continue to use geng from nauty to generate the graph data we are studying. For example, to colour all non-isomorphic, connected, simple graphs on three vertices and count the colour used:\n$ geng -qc 3 | edge_colouring.py\n2\n3\nSo, of the two graphs in question, one (\\(P_{3}\\)) has been coloured with two colours and the other (\\(K_{3}\\)) has been coloured with three colours.\nAs with vertex-colouring, the minimum number of colours in a proper edge-colouring of a graph \\(G\\) is \\(\\Delta(G)\\). In contrast, though, by Vizing’s theorem, at most one extra colour is required.\nTheorem (Vizing)\n\n\\(\\chi^{\\prime}(G) \\leq 1 + \\Delta(G)\\)\n\nA graph \\(G\\) for which \\(\\chi^{\\prime}(G) = \\Delta(G)\\) is called Class One. If \\(\\chi^{\\prime}(G) + 1\\) then \\(G\\) is called Class Two. By Vizing’s theorem every graph is Class One or Class Two. \\(P_{3}\\) is an example of a graph that is Class One and \\(K_{3}\\) is an example of a Class Two graph.\nVizing’s theorem says nothing, however, about how many colours our greedy colouring program will use. We might, though, consider it moderately successful were it to use not many more than \\(\\Delta(G)\\) colours on average.\nSo we are going to consider the total number of colours used to colour all graphs of order \\(n\\) as a proportion of the total maximum degree over the same set of graphs.\nTo compute total number of colours used we follow this tip on summing values in the console using paste and bc:\n$ geng -qc 3\n | edge_colouring.py\n | paste -s -d+\n | bc\n5\nTo compute maximum degrees we depend upon the maxdeg program for gvpr. This means that we have to pipe the output of geng through listg to convert it into DOT format:\n$ geng -qc 3\n | listg -y\n | gvpr -fmaxdeg\nmax degree = 2, node 2, min degree = 1, node 0\nmax degree = 2, node 0, min degree = 2, node 0\nThe output from maxdeg contains much more information than we need and so we need to pipe the output through sed to strip out the maximum degrees:\n$ geng -qc 3\n | listg -y\n | gvpr -fmaxdeg\n | sed -n 's/max degree = \\([0-9]*\\).*/\\1/p'\n2\n2\nNow, piping through paste and bc as before, we find the total over all graphs of the maximum degrees:\n$ geng -qc 3\n | listg -y\n | gvpr -fmaxdeg\n | sed -n 's/max degree = \\([0-9]*\\).*/\\1/p'\n | paste -s -d+\n | bc\n4\nPerhaps surprisingly, with this approach, we find a relatively small discrepancy between the total number of colours used and the total maximum degree. For example, for \\(n = 5\\) (below) the discrepancy is 18 or 25%.\n$ time geng -qc 5\n | edge_colouring.py\n | paste -s -d+\n | bc\n90\n\nreal    0m0.416s\nuser    0m0.328s\nsys 0m0.068s\n$ time geng -qc 5\n | listg -y\n | gvpr -fmaxdeg\n | sed -n 's/max degree = \\([0-9]*\\).*/\\1/p'\n | paste -s -d+\n | bc\n72\n\nreal    0m0.014s\nuser    0m0.004s\nsys 0m0.004s\nFor \\(n = 10\\) the discrepancy is 9189586, or less than 12% of the total of maximum degrees.\n$ time geng -qc 10\n | edge_colouring.py\n | paste -s -d+\n | bc\n87423743\n\nreal    135m6.838s\nuser    131m38.614s\nsys 0m12.305s\n$ time geng -qc 10\n | listg -y\n | gvpr -fmaxdeg\n | sed -n 's/max degree = \\([0-9]*\\).*/\\1/p'\n | paste -s -d+\n | bc\n78234157\n\nreal    48m52.294s\nuser    51m43.042s\nsys 0m12.737s"
  },
  {
    "objectID": "posts/2014-09-26-greedy-edge-colouring-of-small-graphs/index.html#results",
    "href": "posts/2014-09-26-greedy-edge-colouring-of-small-graphs/index.html#results",
    "title": "Greedy Edge Colouring of Small Graphs",
    "section": "Results",
    "text": "Results\nWe repeated the experiment described in the previous section for all values of \\(n\\) from 2 to 10. The results are presented in the plot below which is based on Matplotlib basic plotting from a text file.\n\n\n\nA bar plot with graph order on the x-axis (going from 2 to 10) and number of colours divided by maximum degree on the y-axis (going from 0 to 2). The bar at 2 on the x-axis has height one and therefore the total number of colours used by a greedy strategy for all graphs of order 2 is equal to the total maximum degree over all those graphs. All of the bars are between 1 and 1.5. Apart from bars 2 and 4, all others are greater than 1, indicating that more colours are needed on graphs of those order than the sum of maximum degrees of all graphs of that order. The heighest bars are for orders 3 and 5.\n\n\nFor all orders the total number of colours used by our greedy method is between 1 and 1.5 times the total maximum degree. There also seems to be a tendancy towards a smaller proportion for larger values of \\(n\\). Two theoretical results are relevant here.\nThe first is Shannon’s theorem which concerns the chromatic index of multigraphs:\nTheorem (Shannon)\n\n\\(\\chi^{\\prime}(G) \\leq \\frac{3\\Delta(G)}{2}\\)\n\nShannon’s theorem applies for our experiment because every simple graph is a multigraph with maximum multiplicity 1. An interesting experiment is to see if the results of the above experiment extend to multigraphs. Shannon’s theorem guarantees that for some colouring method it is possible but says nothing about the performance of our specific method.\nA result which is relevant to the second observation, that the proportion tends to 1, concerns the distribution of simple graphs among Class One and Class Two.\nTheorem (10.5 from Chartrand and Zhang (2008))\n\nAlmost every graph is Class One, that is \\(\\lim_{n \\rightarrow \\infty}\\frac{|G_{n,1}|}{|G_{n}|} = 1\\)\n\nwhere \\(G_{n}\\) denotes the set of graphs of order \\(n\\) and \\(G_{n, 1}\\) is the set of Class One graphs of order \\(n\\).\nSo we have good reason to hope that, on average, with larger sets of simple graphs we use fewer colours on average.\nIn the source code section below there is a Drakefile which should reproduce this plot from scratch (provided that the required software is installed)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mjhdata",
    "section": "",
    "text": "Tidy Tuesday: Tornadoes\n\n\n\n\n\n\ndataviz\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: Childcare Costs\n\n\n\n\n\n\ndataviz\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Examples of Maximal Room Squares in R\n\n\n\n\n\n\ncombinatorics\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Text from Images with Google Vision API and R\n\n\n\n\n\n\nocr\n\n\n\n\n\n\n\n\n\nNov 4, 2021\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: Internet Access\n\n\n\n\n\n\ndataviz\n\n\n\nWeek 21 of 2021.\n\n\n\n\n\nMay 11, 2021\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: Water Sources\n\n\n\n\n\n\ndataviz\n\n\n\n\n\n\n\n\n\nMay 5, 2021\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: Adoptable Dogs\n\n\n\n\n\n\ndataviz\n\n\n\nWeek 51 of 2019.\n\n\n\n\n\nDec 17, 2019\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: Replicating Plots in R\n\n\n\n\n\n\ndataviz\n\n\n\nWeek 50 of 2019.\n\n\n\n\n\nDec 10, 2019\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nEuler Paths\n\n\n\n\n\n\ngraph-theory\n\n\n\n\n\n\n\n\n\nOct 17, 2014\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nGreedy Edge Colouring of Small Graphs\n\n\n\n\n\n\ngraph-theory\n\n\n\n\n\n\n\n\n\nSep 26, 2014\n\n\nMatthew Henderson\n\n\n\n\n\n\n\n\n\n\n\n\nMore on Moore Graphs\n\n\n\n\n\n\ngraph-theory\n\n\n\n\n\n\n\n\n\nSep 12, 2014\n\n\nMatthew Henderson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "",
    "text": "In this post I’ll show you how to use Google Cloud Platform’s Vision API to extract text from images in R.\nI’m mostly interested in the case when the text being extracted is handwritten. For typewritten or printed text I use Tesseract.\nIs it feasible to use Vision API for this task? Tesseract works very well with typewritten or printed text but does not seem to handle handwriting as well.[^1] Google’s Vision API, on the other hand, is able to extract handwritten text from images."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#creating-document-text-detection-requests",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#creating-document-text-detection-requests",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "Creating document text detection requests",
    "text": "Creating document text detection requests\nTo use Vision API to extract text from images we must send images to the URL https://vision.googleapis.com/v1/images:annotate in JSON format. Putting images into JSON means encoding them as text. Vision API requires images encoded as text to use the Base64 encoding scheme. (see https://en.wikipedia.org/wiki/Base64 for more details).\nThe JSON payload of a text detection request to Vision API should look something like this:\n{\n  \"requests\": [\n    {\n      \"image\": {\n        \"content\": &lt;encoded-image&gt;\n      },\n      \"features\": [\n        {\n          \"type\": \"DOCUMENT_TEXT_DETECTION\"\n        }\n      ]\n    }\n  ]\n}\nwhere &lt;encoded-image&gt; is a Base64 encoding of the image we want to extract text from.\nFortunately, you don’t have to figure out how to do the encoding yourself. The R package {base64enc} (Urbanek (2015)) can do that for you.\nThe document_text_detection function below uses base64enc::base64encode to compute a Base64 encoding of the image specified by the input path. It then packs the resulting encoding into a list of the right format for converting to JSON before being posted to the URL above.\ndocument_text_detection &lt;- function(image_path) {\n  list(\n    image = list(\n      content = base64enc::base64encode(image_path)\n    ),\n    features = list(\n      type = \"DOCUMENT_TEXT_DETECTION\"\n    )\n  )\n}\nAs you will be using {httr} (Wickham (2020)) you don’t even have to worry about doing the conversion to JSON yourself. {httr} will do it automatically using {jsonlite} (Ooms (2014)) behind-the-scenes when you make your request.\nNow you are ready to use document_text_detection to create a request based on the image located at: ~/workspace/baree-handwriting-scans/001.png\ndtd_001 &lt;- document_text_detection(\"~/workspace/baree-handwriting-scans/001.png\")\nBe wary of inspecting the return value of this function call. It contains a huge base64 string and R might take a very long time to output the string to the screen.\nYou can use substr to inspect part of it, if you really want.\nsubstr(dtd_001, 1, 200)\n#&gt; [1] \"list(content = \\\"iVBORw0KGgoAAAANSUhEUgAAE2EAABtoCAIAAAA8pmPCAAAAA3NCSVQICAjb4U/gAAAACXBIWXMAAFxGAABcRgEUlENBAAAgAElEQVR4nOzdwW7aQBRA0dB/9E8yH+kuUC1qCJiUxFx6zgIJMRo/GxaJzBWH4/E4xvj4+Dg9nkzTNE3T+dPVgu3O\"\n#&gt; [2] \"list(type = \\\"DOCUMENT_TEXT_DETECTION\\\")\"\nAnother pitfall to be wary of is accidentally encoding the path to a file instead of the contents of the file itself. If the response from the Vision API has error messages containing paths then this can be a possible cause."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#posting-requests-to-vision-api",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#posting-requests-to-vision-api",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "Posting requests to Vision API",
    "text": "Posting requests to Vision API\nNow you can use httr:POST from {httr} to post your request to the Vision API.\nhttr::POST requires at least url and body arguments.\nurl is the webpage to be retrieved. In this case \"https://vision.googleapis.com/v1/images:annotate\"\nbody is the request payload. In this case a named list with one element named requests whose value is a list of request objects.\nAs well as those required arguments you also have to tell the server that the content of your request is JSON. You do this by adding “Content-Type: application/json” to the header of your request. This means passing a call to httr::content_type_json() as one of the optional arguments to httr::POST.\nThe Vision API documentation says that for a request to be accepted it must have an Authorization = \"Bearer &lt;GCLOUD_ACCESS_TOKEN&gt;\" header. We can add the header using the httr::add_headers function.\nhttr::add_headers(\n    \"Authorization\" = paste(\"Bearer\", Sys.getenv(\"GCLOUD_ACCESS_TOKEN\"))\n)\nHere we use Sys.getenv(\"GCLOUD_ACCESS_TOKEN\") to obtain the value of our access token.\nThe post_vision function below takes a requests list as input and returns the response from Vision API’s annotation endpoint as a list.\npost_vision &lt;- function(requests) {\n  httr::POST(\n    url = \"https://vision.googleapis.com/v1/images:annotate\",\n    body = requests,\n    encode = \"json\",\n    httr::content_type_json(),\n    httr::add_headers(\n      \"Authorization\" = paste(\"Bearer\", Sys.getenv(\"GCLOUD_ACCESS_TOKEN\"))\n    )\n  )\n}\nFinally, you are ready to use post_vision to make your first request.\npost_vision expects a list of requests as input. In this case you only have one request dtd_001 which you created earlier by calling document_text_detection with the path to your image. Nevertheless, you still have to pack your single request inside a list.\nl_r_001 &lt;- list(requests = dtd_001)\nCalling post_vision now sends your image to Vision API and returns the response.\nr_001 &lt;- post_vision(l_r_001)\nYou can use httr::status_code to check that a valid response was received.\nhttr::status_code(r_001)\n#&gt; [1] 200\nThe value should be 200.\nIf you get a 401 instead then inspect the value of r_001. If you see something like this:\nResponse [https://vision.googleapis.com/v1/images:annotate]\n  Date: 2021-10-29 09:29\n  Status: 401\n  Content-Type: application/json; charset=UTF-8\n  Size: 634 B\n{\n  \"error\": {\n    \"code\": 401,\n    \"message\": \"Request had invalid authentication credentials. Expected OAuth 2 acc...\n    \"status\": \"UNAUTHENTICATED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n        \"reason\": \"ACCESS_TOKEN_EXPIRED\",\n        \"domain\": \"googleapis.com\",\nthen you might need to rerun the gcloud tool (see section 4 of the appendix) to generate a new access token.\nIn the next section I’ll explain how to get text out of a valid response."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#getting-text-from-the-response",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#getting-text-from-the-response",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "Getting text from the response",
    "text": "Getting text from the response\nIf the response is valid then you can use httr::content to extract the content of the response.\ncontent_001 &lt;- httr::content(r_001)\nThe content contains a lot of information. The text is contained inside responses[[1]] as fullTextAnnotation$text.\nbaree_hw_001 &lt;- content_001$responses[[1]]$fullTextAnnotation$text\ncat(baree_hw_001)\n#&gt; a\n#&gt; vast\n#&gt; fear\n#&gt; To Parce, for many days after he was\n#&gt; born, the world was a\n#&gt; gloony cavern.\n#&gt; During these first days of his life his\n#&gt; home was in the heart of a great windfall\n#&gt; where aray wolf, his blind mother, had found\n#&gt; a a safe nest for his hahy hood, and to which\n#&gt; Kazan, her mate, came only now and then ,\n#&gt; his eyes gleaming like strange balls of greenish\n#&gt; fire in the darknen. It was kazan's eyes that\n#&gt; gave\n#&gt; do Barce his first impression of something\n#&gt; existing away from his mother's side, and they\n#&gt; brought to him also his discovery of vision. He\n#&gt; could feel, he could smell, he could hear - but\n#&gt; in that black pirt under the fallen timher he\n#&gt; had never seen until the\n#&gt; eyes\n#&gt; came. At first\n#&gt; they frightened nin; then they puzzled him , and\n#&gt; bis Heer changed to an immense ceniosity, the world\n#&gt; be looking foreight at them when all at once\n#&gt; they world disappear. This was when Kazan turned\n#&gt; his head. And then they would flash hach at him\n#&gt; again wt of the darknen with such startling\n#&gt; Suddenness that Baree world involuntanty Shrink\n#&gt; closer to his mother who always treunded and\n#&gt; Shivered in a strenge way when Kazan came in.\n#&gt; Barce, of course, would never know their story. He\n#&gt; world never know that Gray Wolf, his mother, was\n#&gt; a full-hlooded wolf, and that Kazan, his father,\n#&gt; was a dog. In hin nature was already\n#&gt; nature was already beginning\n#&gt; its wonderful work, but it world never go beyind\n#&gt; cerria limitations. It wald tell him, in time, ,\n#&gt; that his heavtiful wolf - mother was blind, hur\n#&gt; he world never know of that terrible hattle between\n#&gt; Gray Wolf and the lynx in which his mother's sight\n#&gt; had been destroyed Nature could tell hin gatting\n#&gt; nothing\n#&gt; +\n#&gt; а\n#&gt; (\nA lot of the text is readable but there is a lot to do to fix all of the errors. And this is only one page! Maybe things would be better for someone with better handwriting than me.\nIn the next section I’ll try to quantify how close the text extracted by Vision API from the handwritten pages is to the original text by measuring edit distance to the equivalent page of text from Project Gutenberg."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#how-well-does-it-work",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#how-well-does-it-work",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "How well does it work?",
    "text": "How well does it work?\nTo download the original text from Project Gutenberg use the {gutenbergr} (Robinson (2020)) package.\nbaree &lt;- gutenbergr::gutenberg_download(4748)\nIf you get an error here, try a different mirror.\nFrom the text of the entire novel we can extract just the portion that corresponds to our scanned pages. In this case that means lines 96 to 148.\nbaree_tx &lt;- paste(baree$text[96:148], collapse = \" \")\nWith a bit more close inspection we can find the substring of the downloaded text corresponding to the first handwritten page. In this case the substring beginning at the 1st character and ending at the 1597th.\ncat(baree_tx_001 &lt;- stringr::str_sub(baree_tx, 1, 1597))\n#&gt; To Baree, for many days after he was born, the world was a vast gloomy cavern.  During these first days of his life his home was in the heart of a great windfall where Gray Wolf, his blind mother, had found a safe nest for his babyhood, and to which Kazan, her mate, came only now and then, his eyes gleaming like strange balls of greenish fire in the darkness. It was Kazan's eyes that gave to Baree his first impression of something existing away from his mother's side, and they brought to him also his discovery of vision. He could feel, he could smell, he could hear--but in that black pit under the fallen timber he had never seen until the eyes came. At first they frightened him; then they puzzled him, and his fear changed to an immense curiosity. He would be looking straight at them, when all at once they would disappear. This was when Kazan turned his head. And then they would flash back at him again out of the darkness with such startling suddenness that Baree would involuntarily shrink closer to his mother, who always trembled and shivered in a strange sort of way when Kazan came in.  Baree, of course, would never know their story. He would never know that Gray Wolf, his mother, was a full-blooded wolf, and that Kazan, his father, was a dog. In him nature was already beginning its wonderful work, but it would never go beyond certain limitations. It would tell him, in time, that his beautiful wolf mother was blind, but he would never know of that terrible battle between Gray Wolf and the lynx in which his mother's sight had been destroyed. Nature could tell him nothing\nNow using the {stringdist} (der Loo (2014)) package calculate the edit distance between the text extracted from the handwritten page by Vision API and the string extracted from the Project Gutenberg text.\nstringdist::stringdist(baree_hw_001, baree_tx_001, method = \"lv\")\n#&gt; [1] 174\nApparently we could change the handwriting based text to match the text from Project Gutenberg with 174 changes. Quite a lot for one page! However, bear in mind that edit distance is not quite the same as the number of changes you need to make to fix the document yourself. You could use spell check tools to fix some errors quickly and use search-and-replace to remove systematic errors like non-alphabetic characters (assuming your text is entirely alphabetic) or additional spacing.\nIn calculating edit distance we used the default optimal string alignment method from {stringdist}. But the result is the same if we use different methods like Levenshtein distance (method = \"lv\") or Full Damerau-Levenshtein distance (method = \"dl\")."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#building-a-request-based-on-multiple-images",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#building-a-request-based-on-multiple-images",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "Building a request based on multiple images",
    "text": "Building a request based on multiple images\nBegin by putting images of all pages to be converted into the same folder. Here I’ve used the folder ~/workspace/baree-handwriting-scans.\nscans_folder &lt;- \"~/workspace/baree-handwriting-scans\"\nNow use list.files with full.names = TRUE and pattern = '*.png' (assuming your images are PNG format) to get a list of all images.\nscans &lt;- list.files(scans_folder, pattern = '*.png', full.names = TRUE)\nNext, iterate over scans with purrr::map and document_text_detection to create a list of JSON request objects, one for each page.\nscans_dtd &lt;- purrr::map(scans, document_text_detection)\nAs before, wrap this list of requests in another list.\nl_r &lt;- list(requests = scans_dtd)\nBefore calling post_vision and sending all images to Vision API.\nresponse &lt;- post_vision(l_r)\nNotice that even though there are multiple images we still only make one request. The JSON payload contains encodings of all images.\nDepending on the number of images in the requests list the above call to post_vision may take a few seconds to return a response.\nAs before, check that a valid response was received before opening it up and looking at what is inside.\nhttr::status_code(response)\n#&gt; [1] 200\nIf the response is valid you can iterate through the responses list inside the response content extracting fullTextAnnotation from each element.\nThis can be done with purrr::map by passing the name \"fullTextAnnotation\" as the function argument.\nresponses_annotations &lt;- purrr::map(httr::content(response)$responses, \"fullTextAnnotation\")\npurrr::map converts \"fullTextAnnotation\" into an extractor function which pulls out elements of the list argument having that name.\nUsing the same feature of purrr::map you can reach inside the annotations and pull out any text elements. This time using purrr::map_chr instead of purrr:map because the output should be a string.\npurrr::map_chr(responses_annotations, \"text\")\n#&gt; [1] \"a\\nvast\\nfear\\nTo Parce, for many days after he was\\nborn, the world was a\\ngloony cavern.\\nDuring these first days of his life his\\nhome was in the heart of a great windfall\\nwhere aray wolf, his blind mother, had found\\na a safe nest for his hahy hood, and to which\\nKazan, her mate, came only now and then ,\\nhis eyes gleaming like strange balls of greenish\\nfire in the darknen. It was kazan's eyes that\\ngave\\ndo Barce his first impression of something\\nexisting away from his mother's side, and they\\nbrought to him also his discovery of vision. He\\ncould feel, he could smell, he could hear - but\\nin that black pirt under the fallen timher he\\nhad never seen until the\\neyes\\ncame. At first\\nthey frightened nin; then they puzzled him , and\\nbis Heer changed to an immense ceniosity, the world\\nbe looking foreight at them when all at once\\nthey world disappear. This was when Kazan turned\\nhis head. And then they would flash hach at him\\nagain wt of the darknen with such startling\\nSuddenness that Baree world involuntanty Shrink\\ncloser to his mother who always treunded and\\nShivered in a strenge way when Kazan came in.\\nBarce, of course, would never know their story. He\\nworld never know that Gray Wolf, his mother, was\\na full-hlooded wolf, and that Kazan, his father,\\nwas a dog. In hin nature was already\\nnature was already beginning\\nits wonderful work, but it world never go beyind\\ncerria limitations. It wald tell him, in time, ,\\nthat his heavtiful wolf - mother was blind, hur\\nhe world never know of that terrible hattle between\\nGray Wolf and the lynx in which his mother's sight\\nhad been destroyed Nature could tell hin gatting\\nnothing\\n+\\nа\\n(\\n\"\n#&gt; [2] \"7\\n9\\n49\\n7\\nof Kazan's merciless vengeance 1 of the wonderful\\nyears of their matehood of their loyalty, their\\nShenge adventures in the great Canadian wilderness\\ncit'culd make him arby a son of hazar.\\nBut at first, and for many days, it was all\\nMother. Even after his eyes opened wide and he\\nhad pund his legs so that he could shonhce around\\na little in the darkness, nothing existed ar buree\\nfor\\nhut his mother. When he was old enough to he\\n.\\nplaying with Shicks and mess art in the sunlight,\\nhe still did not know what she looked like. But\\nto him she was big and soft and warm, and she\\nhicked his face with her tongue, and talked to him\\nin a gentle, whimpening way that at lost made\\nhim find his own voice in a faint, squeaky yap.\\nAnd then came that wonderful day when the\\ngreenish balls of fire that were kažan's eyes cancie\\nnearer and nearer, a little at a tine, ,\\ncarbiesky. Hereto pore Gray Wolf had warned hin\\nhach. To he alone was the first law of her wild\\nbreed during mothering time. A low snart from her\\n. A\\nthroat, ånd Kazan' had always stopped. But\\nănd\\non this day the snart did not come in aray\\nWolf's throat it died away in a low, whimpering\\nscond. A note of loneliness, of\\ni 아\\ngreat yearniny _“It's all night law,\\\" she was\\nť keys, of a\\nnow\\nsaying to kázan; and katan\\npowsing for a moment\\nreplied with an answłni\\nwswering\\ndeep in his throat.\\nStill slowly, as it not quite sure of what he\\nЕ\\nwould find, Kazan came to them, and Baree\\nsnuggled closer to his mother\\nas he dropped down heavily on his belly close to\\naray Wolf. He was unafraid\\nand nightily\\nand\\nvery\\n.\\nC\\nto make ure -\\nnote\\nHe heard kazan\\n\"\nThe folder_to_chr function below puts all the above steps together.\nInput to folder_to_chr is a path to a folder of images whose text we want to extract. The return value is list of strings containing all text contained in those images.\nfolder_to_chr &lt;- function(scans_folder) {\n  scans &lt;- list.files(scans_folder, pattern = \"*.png\", full.names = TRUE)\n  response &lt;- post_vision(list(requests = purrr::map(scans, document_text_detection)))\n  responses_annotations &lt;- purrr::map(httr::content(response)$responses, \"fullTextAnnotation\")\n  purrr::map_chr(responses_annotations, \"text\")\n}\nFor example, to convert all the pages in scans_folder and combine the results into a single string do something like this:\nbaree_hw &lt;- paste(folder_to_chr(scans_folder), collapse = \" \")"
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#how-well-does-it-work-1",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#how-well-does-it-work-1",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "How well does it work?",
    "text": "How well does it work?\nAs before you can use the edit distance between baree_hw and the equivalent pages of the original text. Here, that text has been extracted before into baree_tx.\nstringdist::stringdist(baree_hw, baree_tx)\n#&gt; [1] 446\nIs this high? It’s hard to know without something to compare against.\nFor example, how does it compare to the edit distance between a random string of the same length as the target text and the target text itself?\nrandom_text &lt;- paste(sample(c(letters, \" \"), stringr::str_length(baree_tx), replace = TRUE), collapse = \"\")\nstringdist::stringdist(random_text, baree_tx)\n#&gt; [1] 2882\nIt’s hardly surprising that this is a much higher number.\nAnother comparison we could make is against the result of using Tesseract to extract text from the same handwritten pages.\nFortunately, the {tesseract} (Ooms (2021)) package makes this very easy.\nfolder_to_chr_ts &lt;- function(scans_folder) {\n  eng &lt;- tesseract::tesseract(\"eng\")\n  scans &lt;- list.files(scans_folder, pattern = \"*.png\", full.names = TRUE)\n  paste(purrr::map(scans, tesseract::ocr, engine = eng), collapse = \" \")\n}\n\nbaree_hw_ts &lt;- folder_to_chr_ts(scans_folder)\nstringdist::stringdist(baree_hw_ts, baree_tx)\n#&gt; [1] 1502\nSo Vision API does much better than Tesseract according to this particular little test. But this is probably not a fair comparison. Tesseract doesn’t claim to be able to extract text from images of handwriting. Furthermore, it may be possible to configure Tesseract in a way that improves the results or to modify the input images to make them better suited to Tesseract. Also, I’m only looking at my own handwriting. It might be that another person’s handwriting works better with Tesseract than mine.\nMy objective was just to see if using Vision API made it possible to convert my own handwritten documents into text. While promising it seems that I might spend longer fixing errors in the resulting text than it would take to type them out from scratch."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#install-the-cloud-sdk",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#install-the-cloud-sdk",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "0. Install the cloud SDK",
    "text": "0. Install the cloud SDK\nYou need to install the glcoud tool to configure authentication.\nThe installation instructions are here: https://cloud.google.com/sdk/docs/install"
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#create-project",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#create-project",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "1. Create project",
    "text": "1. Create project\nFirst you need to create a GCP project and enable Vision API for that project.\nYou will have to setup billing when creating the project if you haven’t done so before.\n\nGo to the Project Selector: https://console.cloud.google.com/projectselector2/home/dashboard and select Create Project\nGive your project a name and click Create.\nEventually the Project Dashboard appears. At the time of writing somewhere in the bottom-left of the dashboard is a Getting Started panel containing a link named Explore and enable APIs. Click on it.\nYou will be transported to the API dashboard.\nAt the top you should see + ENABLE APIs AND SERVICES. Click on that.\nNow you get a search dialog.\nType vision and press enter.\nSelect Cloud Vision API and on the page that appears click the Enable button."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#create-service-account",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#create-service-account",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "2. Create service account",
    "text": "2. Create service account\nThen you need to create a service account.\nThis is a good point to just give up.\nStrictly speaking this isn’t necessary but it does seem to be the most straightforward way of enable authentication for your project.\n\nClick the hamburger in the navigation bar to open the sidebar menu.\nScroll down to IAM & Admin and select Service Accounts.\nClick on Create Service Account.\nGive your account a name.\nClick Create and continue"
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#download-private-key",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#download-private-key",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "3. Download private key",
    "text": "3. Download private key\n\nIn the Service accounts dashboard find the service account you created and click on the three dots below Action.\nSelect Manage Keys from the drop-down menu.\nOn the page that open click on the ADD KEY button.\nChoose Create New Key from the drop-down menu.\nClick Create on the modal dialog that opens.\nYou will be prompted to save your key.\nDownload your private key and remember where you save it (the save location is referenced below as &lt;PATH_TO_PRIVATE_KEY&gt;)."
  },
  {
    "objectID": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#use-gcloud-tool-to-get-access-token",
    "href": "posts/2021-11-04-extracting-text-from-images-with-google-vision-api-and-r/index.html#use-gcloud-tool-to-get-access-token",
    "title": "Extracting Text from Images with Google Vision API and R",
    "section": "4. Use gcloud tool to get access token",
    "text": "4. Use gcloud tool to get access token\nNow you can use gcloud to get the access token required in the above tutorial.\nFirst, define an environment variable GOOGLE_APPLICATION_CREDENTIALS pointing to the location where you saved the private key.\n$ export GOOGLE_APPLICATION_CREDENTIALS=&lt;PATH_TO_PRIVATE_KEY&gt;\nThen run\n$ gcloud auth application-default print-access-token\nThe output will be your access token followed by lots of dots."
  },
  {
    "objectID": "posts/2023-05-04-generating-examples-of-maximal-room-squares-in-r/index.html",
    "href": "posts/2023-05-04-generating-examples-of-maximal-room-squares-in-r/index.html",
    "title": "Generating Examples of Maximal Room Squares in R",
    "section": "",
    "text": "A Room square of order \\(n\\) and side \\(n − 1\\) on an \\(n\\)‐element set \\(S\\) is an \\(n - 1 \\times n - 1\\) array filled with \\(n\\) different symbols in such a way that:\nA partial Room square of order \\(n\\) and side \\(n − 1\\) on an \\(n\\)‐element set \\(S\\) is an \\(n - 1 \\times n - 1\\) array satisfying property (1) above, and also\nA partial Room square is maximal if no further pair of elements of \\(S\\) can be placed into any unoccupied cell of \\(F\\) without violating the conditions (1), (4), (5)."
  },
  {
    "objectID": "posts/2023-05-04-generating-examples-of-maximal-room-squares-in-r/index.html#greedy1",
    "href": "posts/2023-05-04-generating-examples-of-maximal-room-squares-in-r/index.html#greedy1",
    "title": "Generating Examples of Maximal Room Squares in R",
    "section": "greedy1",
    "text": "greedy1\nThe algorithm greedy1 visits each cell in a predetermined order and places the first available pair of symbols into the cell, provided that doing so does not violate the conditions of creating a partial Room square.\nR &lt;- greedy1(6)\nplot_room_square_labs(R)\n\nIn this plot, the colors indicate the sequence in which the cells were filled. Specifically, lighter colors represent cells that were filled earlier in the process, while darker colors represent cells that were filled later.\nis_maximal_proom(R)\n#&gt; [1] TRUE\nHere are a few more examples of maximal partial Room squares created by greedy1."
  },
  {
    "objectID": "posts/2023-05-04-generating-examples-of-maximal-room-squares-in-r/index.html#greedy2",
    "href": "posts/2023-05-04-generating-examples-of-maximal-room-squares-in-r/index.html#greedy2",
    "title": "Generating Examples of Maximal Room Squares in R",
    "section": "greedy2",
    "text": "greedy2\nThe algorithm greedy2 iterates through all pairs of symbols in a predetermined order and places the next available pair into the first empty cell, provided that doing so does not violate the conditions of creating a partial Room square.\nR &lt;- greedy2(6)\nplot_room_square_labs(R)\n\nis_maximal_proom(R)\n#&gt; [1] TRUE\nHere are a few more examples of maximal partial Room squares created by greedy2."
  },
  {
    "objectID": "posts/2014-09-12-more-on-moore-graphs/index.html",
    "href": "posts/2014-09-12-more-on-moore-graphs/index.html",
    "title": "More on Moore Graphs",
    "section": "",
    "text": "In Small Moore Graphs we developed moore, a filter for Moore graphs in graph6 format. The virtue of a program like moore is that it can be used in pipelines with existing programs to create new programs, as demonstrated in that earlier post.\nIn its present form (at time of writing) moore filters Moore graphs from a string of whitespace delimited graphs in graph6 format. So, to use it in a pipeline we have to ensure that the input is a single string, rather than raw standard input:\nBeyond this small design flaw, moore has a few other, as yet unresolved, issues. For example, it fails to filter the Moore graphs of order seven from a string of all non-isomorphic connected graphs on seven vertices.\nRather than fix these problems immediately, in this post, we build an alternative implementation of the same program. Before, with moore we used Bash and Maxima. Here we will Python with both the NetworkX and igraph packages. The former for its graph6 handling and the latter for degree, girth and diameter calculations."
  },
  {
    "objectID": "posts/2014-09-12-more-on-moore-graphs/index.html#iterating-over-graphs-with-python",
    "href": "posts/2014-09-12-more-on-moore-graphs/index.html#iterating-over-graphs-with-python",
    "title": "More on Moore Graphs",
    "section": "Iterating Over Graphs with Python",
    "text": "Iterating Over Graphs with Python\nThe resulting program, moore.py will read graphs in graph6 format from standard input and echo back those graphs which are Moore graphs.\nOne approach to working with standard input in Python is to use the stdin object from the sys module of the standard library. The stdin object has a readlines method that makes iterating over lines of standard input as simple as:\nfrom sys import stdin\n\nfor line in stdin.readlines():\n    # Do something\nWe will expect here that each line is a graph6 format string. Inside the body of the loop we then need to do the following three things:\n\nparse the graph6 string into a graph object G,\ncheck if G is Moore graph or not and, if it is,\necho the original input line on standard output.\n\nThe first of these steps can be handled by the parse_graph6 function from NetworkX. The only processing we do on each line is to strip whitespace on the right using the rstrip string method.\nThe result of parsing is a networkx.Graph object g. As NetworkX does not implement girth computation we construct a second igraph.Graph object G from g.edges(), the list of edges of g.\nfrom sys import stdin\n\nfrom networkx import parse_graph6\n\nfrom igraph import Graph\n\nif __name__ == \"__main__\":\n    for line in stdin.readlines():\n        stripped_line = line.rstrip()\n        g = parse_graph6(stripped_line)\n        G = Graph(edges = g.edges())\n        moore = moore_gd\n        if moore(G):\n            print stripped_line\nTesting for Moore graphs is done by a function moore (here pointing to one of three alternative implementations moore_gd, moore_nd and moore_gn). In the next section these three different functions are described."
  },
  {
    "objectID": "posts/2014-09-12-more-on-moore-graphs/index.html#testing-moore-graphs",
    "href": "posts/2014-09-12-more-on-moore-graphs/index.html#testing-moore-graphs",
    "title": "More on Moore Graphs",
    "section": "Testing Moore Graphs",
    "text": "Testing Moore Graphs\nAs seen in Small Moore Graphs there are, at least, three different ways to test whether a graph is a Moore graph or not. Those three methods are based on a theorem from Cameron (1994) which says that a graph is a Moore graph if it satisfies any two of the following three conditions:\n\n\\(G\\) is connected with maximum degree \\(k\\) and diameter \\(d\\);\n\\(G\\) has minimum degree \\(k\\) and girth \\(2d + 1\\);\n\\(G\\) has \\(1 + k\\sum_{i=0}^{d-1}(k - 1)^{i}\\) vertices.\n\nThe third condition gives the maximum order of a \\(k\\)-regular graph with diameter \\(d\\). As this is a value we need in more than one place it gets its own function.\ndef moore_order(d, k):\n    \"\"\"\n    In a regular graph of degree k and diameter d the order is\n    at most moore_order(d, k).\n    \"\"\"\n    return 1 + k*sum([(k - 1)**i for i in range(d)])\nNow moore_gn, which is based on the combination of conditions 2 (involving girth) and 3 (involving order) above can be implemented for igraph.Graph objects as follows:\ndef moore_gn(G):\n  \"\"\"\n  Decide if G is a Moore graph or not, based on order and girth.\n  \"\"\"\n  return G.vcount() == moore_order((G.girth() - 1)/2, min(G.degree()))\nRemembering that every graph which satisfies conditions 2 and 3 above is also regular and connected might persuade us to consider some optimisations here. For example, as the minimum degree of vertices must be calculated we might as well also compute the maximum degree and avoid moore_order and girth calculations for any graph for which those values differ.\nSimilarly, we might also dismiss any graph which isn’t connected. Optimisations like these require some experimentation to determine their worth. Also, when programs like geng have already efficient ways to generated connected and regular graphs there will be circumstances when we only want the essential computation to be done. So at present we will concentrate on building a reliable implementation and leave such considerations for the future.\nWith disregard for optimisation in mind, the other testing functions based on the remaining combinations of conditions 1, 2 and 3. are also very simple one-liners. The girth and diameter variant looks like:\ndef moore_gd(G):\n  \"\"\"\n  Decide if G is a Moore graph or not, based on girth and diameter.\n  \"\"\"\n  return G.girth() == 2*G.diameter() + 1\nWhile the version based on order and diameter is:\ndef moore_nd(G):\n  \"\"\"\n  Decide if G is a Moore graph or not, based on order and diameter.\n  \"\"\"\n  return G.vcount() == moore_order(G.diameter(), max(G.degree()))"
  },
  {
    "objectID": "posts/2014-09-12-more-on-moore-graphs/index.html#results",
    "href": "posts/2014-09-12-more-on-moore-graphs/index.html#results",
    "title": "More on Moore Graphs",
    "section": "Results",
    "text": "Results\nNow we can construct all Moore graphs on at most 10 vertices in a single pipeline involving geng and moore.py. Here the resulting graphs are visualised with circo from Graphviz after conversion to DOT format using listg:\n$ options=\"-Gsize=5,5!\n           -Nfixedsize=true\n           -Nlabel=\n           -Nshape=circle\n           -Nheight=0.2\n           -Nwidth=0.2\n           -Nstyle=filled\n           -Nfillcolor=black\"\n\n$ seq 1 10\\\n  | xargs -L1 geng -qc\\\n  | moore.py\\\n  | listg -y\\\n  | circo -Tsvg -O $options"
  },
  {
    "objectID": "posts/2021-05-11-tidy-tuesday-internet-access/index.html",
    "href": "posts/2021-05-11-tidy-tuesday-internet-access/index.html",
    "title": "Tidy Tuesday: Internet Access",
    "section": "",
    "text": "This week’s Tidy Tuesday data comes from two sources, Microsoft and the FCC by way of The Verge.\nAccording to The Verge, the FCC data, which dates from the end of 2017, is\n\na notoriously inaccurate survey drawn from ISPs’ own descriptions of the areas they serve.\n\nMicrosoft estimate connection speeds from throughput of software updates.\n\nWe know the size of the package sent to the computer, and we know the total time of the download. We also determine zip code level location data via reverse IP. Therefore, we can count the number of devices that have connected to the internet at broadband speed per each zip code based on the FCC’s definition of broadband that is 25mbps per download.\n\nMicrosoft’s data is from November 2019.\nI chose to compare these two different measures of connection speed at county level for one specific state, Kentucky.\nTo plot county boundaries I used the {tigris} package, an R package for downloading geographic data from the United States Census Bureau.\nThis was first time I can remember using {tigris}. It made plotting county boundaries a breeze.\n{tigris} can also be used to plot other data from the Census Bureau. I got a bit distracted plotting maps of Kentucky roads.\nFor example, to download all roads in Madison County and plot them with {ggplot2} you can do the following:\nlibrary(ggplot2)\nlibrary(tigris)\n\nmadison_ky_roads &lt;- roads(\"KY\", \"Madison\", progress_bar = FALSE)\n\nmadison_ky_roads %&gt;%\n  ggplot() +\n  geom_sf(size = .1, alpha = .8) +\n  theme_void()\n\nIn the final plot below there are two maps of the counties of Kentucky.\nIn the top map, colours correspond to the proportion of residents in a county that have access to broadband internet, according to the FCC.\nThe colouring in the bottom map represents estimates of the same proportion, according to Microsoft.\nThe two plots differ substantially.\nAccording to the first plot, the proportion of residents having access to broadband internet appears to be high in most counties.\nHowever, the second plot suggests that the actual proportion of people in counties using the internet at broadband speed is much lower, except perhaps in the vicinity of cities like Lexington, Louisville, Bowling Green and Cincinnati.\nSource code: https://github.com/MHenderson/internet-access\n\n\n\nA plot comparing different ways of measuring internet access for people living in Kentucky"
  },
  {
    "objectID": "posts/2019-12-10-tidy-tuesday-replicating-plots-in-r/index.html",
    "href": "posts/2019-12-10-tidy-tuesday-replicating-plots-in-r/index.html",
    "title": "Tidy Tuesday: Replicating Plots in R",
    "section": "",
    "text": "Tidy Tuesday in week fifty of 2019 was all about this blog post by Rafael Irizarry on replicating plots in R.\nI chose to focus on a heatmap showing infectious diseases in the United States before and after the introduction of vaccines.\nMy plot is heavily based on Rafael’s code with just a few extra annotations and a change of scale to match the original version published in the Wall Street Journal.\nThis was my first attempt at adding annotations outside the main plotting area. I learned how to create the annotations I wanted for this plot from the Tidy Tuesday work of Georgios Karamanis. Especially his week 36 entry for 2021.\nMy code is here: https://github.com/MHenderson/replicating-plots-in-r\n\n\n\nPlot shows a heat map of measles cases per one hundred thousand people measured from 1928 to 2003 across all fifty US states and the District of Columbia. Showing that after 1963 when measles vaccine was introduced there was a dramatic drop in the number of measles cases throughout the United States."
  }
]